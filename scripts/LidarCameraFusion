#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jun 25 18:14:34 2020
@author: elif.ayvali

LidarPoint   {x,y,z in [m], r is point reflectivity}
BoundingBox {
    boxID        :bounding box around a classified object (contains both 2D and 3D data)
    trackID      :unique identifier for the track to which this bounding box belongs
    roi          :cv.Rect 2D region-of-interest in image coordinates
    classID      :ID based on class file provided to YOLO framework
    confidence   :classification trust
            }
DataFrame {
    cameraImg    :color camera image
    keypoints    :2D keypoints within camera image
    descriptors  :keypoint descriptors
    kptMatches   :keypoint matches between previous and current frame
    lidarPoints  :
    boundingBoxes:
    bbMatches    :
          }

"""
import cv2 as cv
from recordtype import recordtype #mutable namedtuple
import glob
import numpy as np
import matplotlib.pyplot as plt
from SensorProcessor import LidarProcessing,CameraProcessing,ObjectDetectionOpenCV
from utils import View, Plot

###########################################################################
#-----------------Define Data Structures and Paths------------------------#
###########################################################################       
#Define data structures
LidarPoint=recordtype('LidarPoint',['x','y','z','r'])
DataFrame=recordtype('DataFrame',['cameraImg','keypoints', 'descriptors' ,'kptMatches','lidarPoints','boundingBoxes','bbMatches'])
BoundingBox=recordtype('BoundingBox',['boxID','trackID','roi','classID', 'confidence','lidarPoints','keypoints','kptMatches'])

#Data location
#Images
img_folderpath='../data/KITTI/2011_09_26/image_02/data/*.png'
img_filepaths=sorted(glob.glob(img_folderpath))
#Lidar
lidar_folderpath='../data/KITTI/2011_09_26/velodyne_points/data/*.bin'
lidar_filepaths=sorted(glob.glob(lidar_folderpath))
#Yolo Network
labelsPath = "../data/YOLO/coco.names"
weightsPath = "../data/YOLO/yolov3.weights"
configPath = "../data/YOLO/yolov3.cfg"



#Loop over all data sequence
dataBuffer=[]
idx_step=1
idx_end=1#len(img_filepaths)
for idx in range(0,idx_end,idx_step):
    ###########################################################################
    #-----------------------------Process Camera------------------------------#
    ###########################################################################   
    img = cv.imread(img_filepaths[idx]) 
    # plt.figure(figsize=(16, 16))
    # plt.title('Front camera')
    # plt.imshow(cv.cvtColor(img1, cv.COLOR_BGR2RGB))
    # plt.show()
    
    #YOLO
    params=dict()
    params["confidence"]=0.2
    params["threshold"]=0.4
    params["shrink"]=0.1
    bBox=BoundingBox('None','None','None','None', 'None','None','None','None')
    frame=DataFrame('None','None', 'None' ,'None','None','None','None')

    YOLO=ObjectDetectionOpenCV(params,labelsPath,weightsPath,configPath,bBox) 
    bBoxes,img_boxes=YOLO.predict(img)    
    frame.cameraImg=img.copy()
    frame.boundingBoxes=bBoxes.copy()
    dataBuffer.append(frame)         
    print("#1 : LOAD IMAGE INTO BUFFER done")
    print("#2 : DETECT & CLASSIFY OBJECTS done")
    
    plt.figure(figsize=(16, 16))
    plt.title('Yolo OnjectClassification')
    plt.imshow(cv.cvtColor(img_boxes, cv.COLOR_BGR2RGB))
    plt.show()
    
    ###########################################################################
    #------------------------------Process Lidar------------------------------#
    ###########################################################################

    velodyne_scan = np.fromfile(lidar_filepaths[idx], dtype=np.float32)
    #import lidar data
    lidar_scan=velodyne_scan.reshape((-1, 4))    
    #-----------------KITTI-------------------#
    #display topview
    x_range,y_range,z_range,scale=(-20, 20),(-20, 20),(-2, 2),10
    img_lidar_topview=View.lidar_top_view_kitti(lidar_scan, x_range, y_range, z_range, scale)#lidar_pts:nx4
    #remove lidar points outside of the field of view and compute color range based on max distance
    v_fov, h_fov,max_d = (-24.9, 2.0), (-90, 90),70
    lidar_pts,lidar_color=LidarProcessing.velo_points_filter_kitti(lidar_scan,v_fov,h_fov,max_d)
    print("#3 : CROP LIDAR POINTS done")
    #project lidar points to camera images
    lidar_px=LidarProcessing.projectLidarToCam(lidar_pts) 
    #Overlay lidar on camera image
    img_lidar_fusion=View.lidar_overlay_plt_kitti(lidar_px, lidar_color, img)
    
    # plt.figure(figsize=(16, 16))
    # plt.title('Top-View Perspective of LiDAR data (Kitti)')
    # plt.imshow(img_lidar_topview)
    # plt.show()
    
    # plt.figure(figsize=(16, 16))
    # plt.title('Lidar Fusion (Kitti)')
    # plt.imshow(img_lidar_fusion)
    # plt.show()

    #-----------------Udacity-------------------#
    img_lidar_topview_udacity=View.lidar_top_view_udacity(lidar_scan)
    #remove Lidar points based on distance properties
    # minZ = -1.5; maxZ = -0.9; minX = 2.0; maxX = 20.0; maxY = 2.0; minR = 0.1; # focus on ego lane
    minZ = -1.4; maxZ = 1e2; minX = 0.0; maxX = 25.0; maxY = 6.0; minR = 0.01;max_d=20;
    crop_range=(minZ,maxZ,minX,maxX,maxY,minR)
    lidar_pts_udacity,lidar_color_udacity=LidarProcessing.crop_lidar_points_udacity(lidar_scan,crop_range,max_d)
    #project lidar points to camera images
    lidar_px_udacity=LidarProcessing.projectLidarToCam(lidar_pts_udacity) 
    #Overlay lidar on camera image
    img_lidar_fusion_udacity=View.lidar_overlay_plt_udacity(lidar_px_udacity, lidar_color_udacity, img)    
    
    # plt.figure(figsize=(16, 16))
    # plt.title('Top-View Perspective of LiDAR data (Udacity)')
    # plt.imshow(img_lidar_topview_udacity)
    # plt.axis('off')
    # plt.show()
    
    # plt.figure(figsize=(16, 16))
    # plt.title(' LiDAR fusion (Udacity)')
    # plt.imshow(img_lidar_fusion_udacity)
    # plt.axis('off')
    # plt.show()
 
    ###########################################################################
    #-------------------------Descriptor Matching-----------------------------#
    ###########################################################################       
    if len(dataBuffer)>1:        
        img_last=np.copy(dataBuffer[-2].cameraImg)#last frame
        img_current=np.copy(dataBuffer[-1].cameraImg)#current frame        
        kp_last,kp_current,des_last,des_current=CameraProcessing.computeFeatureDescriptors(img_last,img_current)
        print("#5-6 : DETECT & EXTRACT KEYPOINTS done") 
        good_matches,good_kp_last,good_kp_current=CameraProcessing.matchFeatureDescriptor(kp_last,kp_current,des_last,des_current)
        num_good_matches = len(good_matches)
        print("#7 : MATCH KEYPOINT DESCRIPTORS done" )        
        cv.drawKeypoints(img_last, good_kp_last,img_last,(0, 0, 255),cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
        cv.drawKeypoints(img_current, good_kp_current,img_current,(0, 0, 255),cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)    
        plt.figure(figsize=(16, 16))
        plt.title('last_frame')
        plt.imshow(cv.cvtColor(img_last, cv.COLOR_BGR2RGB))
        plt.show()        
        plt.figure(figsize=(16, 16))
        plt.title('current_frame')
        plt.imshow(cv.cvtColor(img_current, cv.COLOR_BGR2RGB))
        plt.show()

